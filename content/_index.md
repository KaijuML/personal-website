---
Lastmod: 2021-04-08
sitemap:
  Priority: "1"
  ChangeFreq: "monthly"
---

## About Me

I am an **NLP Researcher**, and **Quant** at [**G-Research**](https://www.gresearch.co.uk/about/).

Before that, I did my **PhD** at **Sorbonne Université**, while being a **Research Engineer** at **BNP Paribas**. My PhD, _Deep Learning for Data-to-Text Generation_ was done under the supervision of [Patrick Gallinari][1] and [Laure Soulier][2], from the [MLIA][3] team. All projects (solos \& duos) are available on [Github][4] and [ArXiv][5].

### Work

I work as a **Quant Researcher**, meaning I research **systematic trading ideas** to predict the future of financial markets, applying scientific techniques to find patterns in large, noisy and rapidly changing realworld data sets. In otherwords, I apply and develop state-of-the-art NLP approaches (read [_transformers_](<https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)>)) to find trading signals in large textual corpora. By making computers do the trading, we remove human error and make sure only rigourously proven-to-work strategies are deployed.

Before that, I was a Research Engineer at BNP Paribas. In practice, I bridged the gap between research/academia and applications/enterprise, being part of the team which developped the internal company-wide search engine, as well as a number of other tools (translation plateform, document NLP, etc.).

### Academic Research

Right now, I am interested in all things NLP.

During my PhD, I worked on **Data-to-Text Generation** (DTG), i.e. building systems able to:

- comprehend complex structured data (e.g. tables, graphs, etc.);
- produce a fitting description (from one sentence to several paragraphs).

These systems are crucial in environments where raw data is abundant, but hardly usable as is (e.g. health, sports, etc.), because end-users are more effective when provided with textual summaries than structured data<sup>1</sup>.

My PhD work has been focused on a critical aspect of DTG: **ensuring factualness in system outputs**. Neural networks have proven shockingly good at producing fluent texts, but end-users care more about accuracy than about readability<sup>2</sup>. Wrong descriptions that must be revised by human experts are of limited utility. In this direction, I have proposed novel encoding neural modules that are better suited for complex structured data, evaluation protocols that can better discriminate between models by leveraging structured data, and training procedures that ensure models don’t pick up on biased human behaviours (such as mentioning unverifiable facts).

In 2021, I have focused on working with other PhD students, with notably fruitful collaborations with University of Turin (Italy), University of Aberdeen (UK), as well as Sorbonne Université (France).

### Hobbies

On a personal note, I am a climbing enthusiast and try to swim at least once per week. I greatly enjoy storytelling, both reading and going to the movies (used to go twice a week w/ movie pass before I moved to London). I'm also a fan of cooking: meals, deserts, as well as **cocktails** :tropical_drink: See the Gallery Section for some proof that I go outside!

<sub>1: From data to text in the Neonatal Intensive Care Unit: Using NLG technology for decision support and information management. Gatt et al. 2009</sub>  
<sub>2: An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems. Belz and Reiter 2009</sub>

[1]: http://www-connex.lip6.fr/~gallinar/gallinari/pmwiki.php
[2]: https://mlia.lip6.fr/soulier/
[3]: https://mlia.lip6.fr/
[4]: https://github.com/KaijuML
[5]: https://arxiv.org/search/cs?searchtype=author&query=Rebuffel%2C+C
